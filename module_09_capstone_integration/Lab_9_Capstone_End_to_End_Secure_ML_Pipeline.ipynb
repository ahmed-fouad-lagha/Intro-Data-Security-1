{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486dae92",
   "metadata": {},
   "source": [
    "# Lab 9: Capstone — End-to-End Secure ML Pipeline\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this capstone, you will be able to:\n",
    "\n",
    "1. **Design** an end-to-end secure ML system (threat model → defenses)\n",
    "2. **Implement** multi-layer defenses across data, training, and inference\n",
    "3. **Evaluate** attacks: evasion, poisoning, membership inference, resource exhaustion\n",
    "4. **Measure** privacy, robustness, and utility trade-offs\n",
    "5. **Operationalize** security with monitoring, rate limits, and alerts\n",
    "\n",
    "---\n",
    "\n",
    "## Capstone Scenario\n",
    "\n",
    "You are the ML security lead for a healthcare imaging startup. The company deploys a\n",
    "cloud-hosted classifier for medical triage. Your system must withstand:\n",
    "\n",
    "- **Evasion attacks:** Adversarial perturbations to fool predictions\n",
    "- **Data poisoning:** Malicious training samples injected into the pipeline\n",
    "- **Membership inference:** Adversaries trying to detect patient data inclusion\n",
    "- **Sponge attacks:** Resource exhaustion via crafted inputs\n",
    "\n",
    "Your job: build a secure pipeline and quantify its performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Threat Model](#threat-model)\n",
    "2. [Secure Data Pipeline](#data)\n",
    "3. [Robust Training (Adversarial + DP)](#training)\n",
    "4. [Secure Inference (Validation + Rate Limiting)](#inference)\n",
    "5. [Attack Suite](#attacks)\n",
    "6. [Evaluation Dashboard](#evaluation)\n",
    "7. [Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## Threat Model <a id=\"threat-model\"></a>\n",
    "\n",
    "| Layer | Threat | Goal | Defense |\n",
    "|------|--------|------|---------|\n",
    "| Data | Poisoning | Corrupt model | Outlier detection, sanitization |\n",
    "| Training | Membership inference | Privacy leakage | DP-SGD |\n",
    "| Inference | Evasion | Misclassification | Adversarial training |\n",
    "| Inference | Sponge attack | DoS/resource exhaustion | Validation + rate limiting |\n",
    "| Ops | Model theft | IP exposure | Logging + anomaly detection |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use subset for faster execution\n",
    "train_indices = np.random.choice(len(train_dataset), 8000, replace=False)\n",
    "test_indices = np.random.choice(len(test_dataset), 2000, replace=False)\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "test_data = Subset(test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a316bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Definition\n",
    "# ============================================================================\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "print('Model ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Secure Data Pipeline: Simple Poisoning + Detection\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Secure Data Pipeline: Label-Flipping + Pixel-Based Detector (Mismatch)')\n",
    "print('='*70)\n",
    "\n",
    "def poison_labels(dataset, poison_rate=0.1, target_label=0):\n",
    "    \"\"\"Flip labels for a fraction of samples (label-flipping attack).\"\"\"\n",
    "    indices = np.random.choice(len(dataset), int(poison_rate * len(dataset)), replace=False)\n",
    "    poisoned = []\n",
    "    for i in range(len(dataset)):\n",
    "        x, y = dataset[i]\n",
    "        if i in indices:\n",
    "            poisoned.append((x, target_label))\n",
    "        else:\n",
    "            poisoned.append((x, y))\n",
    "    return poisoned, indices\n",
    "\n",
    "# Create poisoned dataset\n",
    "poisoned_data, poisoned_indices = poison_labels(train_data, poison_rate=0.1, target_label=0)\n",
    "\n",
    "# NOTE: Label-flipping does not change pixel values.\n",
    "# A pixel-based outlier detector (IsolationForest) is therefore NOT effective here.\n",
    "X_flat = torch.stack([poisoned_data[i][0].view(-1) for i in range(len(poisoned_data))]).numpy()\n",
    "clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_preds = clf.fit_predict(X_flat)\n",
    "\n",
    "outlier_rate = (outlier_preds == -1).mean()\n",
    "print(f'Poisoned samples (label flips): {len(poisoned_indices)}')\n",
    "print(f'Pixel-based outlier rate: {100*outlier_rate:.1f}% (not tied to label flips)')\n",
    "\n",
    "# Keep all samples; no reliable pixel-based filtering for label-flip attacks\n",
    "sanitized_data = poisoned_data\n",
    "print(f'Sanitized dataset size: {len(sanitized_data)} (no pixel-based filtering)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Robust Training: Adversarial + DP-SGD (Simplified)\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Robust Training: Adversarial + DP-SGD')\n",
    "print('='*70)\n",
    "\n",
    "@dataclass\n",
    "class DefenseConfig:\n",
    "    epsilon_adv: float = 0.1\n",
    "    clip_norm: float = 1.0\n",
    "    noise_multiplier: float = 1.0\n",
    "\n",
    "def fgsm_attack(model, data, target, epsilon=0.1):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = data.grad.data\n",
    "    perturbed = data + epsilon * data_grad.sign()\n",
    "    return torch.clamp(perturbed, -3, 3)\n",
    "\n",
    "def dp_sgd_step(model, data, target, config: DefenseConfig):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Per-sample gradients\n",
    "    per_sample_grads = []\n",
    "    for i in range(data.size(0)):\n",
    "        model.zero_grad()\n",
    "        output = model(data[i:i+1])\n",
    "        loss = criterion(output, target[i:i+1]).mean()\n",
    "        loss.backward()\n",
    "        grads = [p.grad.detach().clone() for p in model.parameters()]\n",
    "        per_sample_grads.append(grads)\n",
    "\n",
    "    # Clip and aggregate\n",
    "    agg_grads = []\n",
    "    for param_i in range(len(per_sample_grads[0])):\n",
    "        stacked = torch.stack([g[param_i] for g in per_sample_grads], dim=0)\n",
    "        # Clip\n",
    "        norms = torch.norm(stacked.view(stacked.size(0), -1), dim=1)\n",
    "        factors = torch.clamp(config.clip_norm / (norms + 1e-6), max=1.0)\n",
    "        stacked = stacked * factors.view(-1, *([1] * (stacked.dim()-1)))\n",
    "        agg = stacked.mean(dim=0)\n",
    "        # Noise\n",
    "        noise = torch.randn_like(agg) * (config.noise_multiplier * config.clip_norm / data.size(0))\n",
    "        agg_grads.append(agg + noise)\n",
    "\n",
    "    for param, grad in zip(model.parameters(), agg_grads):\n",
    "        param.grad = grad\n",
    "\n",
    "def train_secure(model, loader, config: DefenseConfig, epochs=3):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            adv = fgsm_attack(model, data, target, epsilon=config.epsilon_adv)\n",
    "            optimizer.zero_grad()\n",
    "            dp_sgd_step(model, adv, target, config)\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Secure training complete')\n",
    "\n",
    "secure_model = SmallCNN().to(device)\n",
    "secure_loader = DataLoader(sanitized_data, batch_size=64, shuffle=True)\n",
    "config = DefenseConfig()\n",
    "train_secure(secure_model, secure_loader, config, epochs=3)\n",
    "\n",
    "secure_train_acc = evaluate(secure_model, train_loader)\n",
    "secure_test_acc = evaluate(secure_model, test_loader)\n",
    "print(f'\\nSecure Model Accuracy: Train={secure_train_acc:.2f}%, Test={secure_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e711b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Secure Inference: Input Validation + Rate Limiting\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Secure Inference: Input Validation + Rate Limiting')\n",
    "print('='*70)\n",
    "\n",
    "@dataclass\n",
    "class InputPolicy:\n",
    "    max_abs_value: float = 3.0\n",
    "    max_variance: float = 2.0\n",
    "\n",
    "def validate_input(x: torch.Tensor, policy: InputPolicy) -> bool:\n",
    "    if x.abs().max().item() > policy.max_abs_value:\n",
    "        return False\n",
    "    if x.var().item() > policy.max_variance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "@dataclass\n",
    "class RateLimiter:\n",
    "    max_qps: int = 100\n",
    "    burst: int = 10\n",
    "    tokens: int = 10\n",
    "\n",
    "    def allow(self):\n",
    "        if self.tokens > 0:\n",
    "            self.tokens -= 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "policy = InputPolicy()\n",
    "limiter = RateLimiter()\n",
    "\n",
    "# Simulate 20 requests\n",
    "allowed = 0\n",
    "for _ in range(20):\n",
    "    if limiter.allow():\n",
    "        allowed += 1\n",
    "\n",
    "print(f'Requests allowed (burst=10): {allowed}/20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Attack Suite\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Attack Suite')\n",
    "print('='*70)\n",
    "\n",
    "def get_confidences(model, loader):\n",
    "    model.eval()\n",
    "    confs = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "            confs.extend(probs.max(dim=1)[0].cpu().numpy())\n",
    "    return np.array(confs)\n",
    "\n",
    "# 1) Evasion (FGSM)\n",
    "def eval_fgsm(model, loader, epsilon=0.1):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        adv = fgsm_attack(model, data, target, epsilon=epsilon)\n",
    "        out = model(adv)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# 2) Membership inference (confidence-based)\n",
    "def membership_auc(model):\n",
    "    train_conf = get_confidences(model, train_loader)\n",
    "    test_conf = get_confidences(model, test_loader)\n",
    "    labels = np.concatenate([np.ones(len(train_conf)), np.zeros(len(test_conf))])\n",
    "    scores = np.concatenate([train_conf, test_conf])\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "# 3) Sponge (simple variance-based input)\n",
    "def sponge_input(scale=5.0):\n",
    "    x = torch.randn(1, 1, 28, 28) * scale\n",
    "    return x\n",
    "\n",
    "# Evaluate secure model\n",
    "clean_acc = evaluate(secure_model, test_loader)\n",
    "fgsm_acc = eval_fgsm(secure_model, test_loader, epsilon=0.1)\n",
    "mia_auc = membership_auc(secure_model)\n",
    "\n",
    "# Sponge rejection\n",
    "sponge = sponge_input(scale=5.0)\n",
    "sponge_allowed = validate_input(sponge, policy)\n",
    "\n",
    "print(f'Clean accuracy: {clean_acc:.2f}%')\n",
    "print(f'FGSM accuracy: {fgsm_acc:.2f}%')\n",
    "print(f'Membership AUC: {mia_auc:.4f}')\n",
    "print(f'Sponge input allowed? {sponge_allowed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Evaluation Dashboard\n",
    "# ============================================================================\n",
    "\n",
    "clean_acc_frac = clean_acc / 100.0\n",
    "fgsm_acc_frac = fgsm_acc / 100.0\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Clean Acc (fraction)', 'FGSM Acc (fraction)', 'MIA AUC', 'Sponge Allowed'],\n",
    "    'Value': [clean_acc_frac, fgsm_acc_frac, mia_auc, 1.0 if sponge_allowed else 0.0]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(results['Metric'], results['Value'], color=['#2ecc71', '#e67e22', '#3498db', '#e74c3c'])\n",
    "ax.set_ylabel('Value (0–1)')\n",
    "ax.set_title('Secure ML Pipeline: Key Metrics')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('capstone_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52066532",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Attack Scaling (Medium)\n",
    "Increase attack strength (ε) and report FGSM accuracy drop.\n",
    "\n",
    "### Exercise 2: Poisoning Budget (Hard)\n",
    "Vary poisoning rate (0%, 5%, 10%, 20%) and measure accuracy + MIA AUC.\n",
    "\n",
    "### Exercise 3: Privacy Budget (Hard)\n",
    "Sweep DP noise multiplier and measure MIA AUC vs test accuracy.\n",
    "\n",
    "### Exercise 4: Sponge Defense Tuning (Medium)\n",
    "Adjust input variance threshold and measure false positives.\n",
    "\n",
    "### Exercise 5: Defense Ablation (Hard)\n",
    "Disable one defense at a time (DP, adversarial training, sanitization).\n",
    "Quantify which defense contributes most to security.\n",
    "\n",
    "### Exercise 6: Deployment Plan (Hard)\n",
    "Write a security checklist for production deployment, including monitoring,\n",
    "logging, and incident response."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
