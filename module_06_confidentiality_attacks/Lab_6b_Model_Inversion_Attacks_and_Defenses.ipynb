{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d115a4",
   "metadata": {},
   "source": [
    "# Lab 6b: Model Inversion Attacks and Defenses\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will understand:\n",
    "\n",
    "1. **Model Inversion:** Reconstructing training data from model parameters/outputs\n",
    "2. **Privacy Extraction:** How gradients leak information about training samples\n",
    "3. **Attack Vectors:** Feature inversion, sample reconstruction, attribute inference\n",
    "4. **Defense Mechanisms:** Differential privacy, gradient clipping, output perturbation\n",
    "5. **Threat Model:** White-box vs black-box inversion attacks\n",
    "6. **Real-World Impact:** Reconstructing faces, names, sensitive attributes from ML models\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Threat Model: Data Reconstruction](#threat-model)\n",
    "2. [Feature Inversion: Basic Attack](#feature-inversion)\n",
    "3. [Gradient-Based Sample Reconstruction](#gradient-reconstruction)\n",
    "4. [Attribute Inference Attack](#attribute-inference)\n",
    "5. [Defense Mechanisms](#defenses)\n",
    "6. [Defense Effectiveness Analysis](#effectiveness)\n",
    "7. [Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## Threat Model: Data Reconstruction <a id=\"threat-model\"></a>\n",
    "\n",
    "**Core Question:** Can an attacker reconstruct training data from model outputs/parameters?\n",
    "\n",
    "### Real-World Scenarios:\n",
    "\n",
    "| Scenario | Attack | Data at Risk | Impact |\n",
    "|----------|--------|--------------|--------|\n",
    "| **Face Recognition** | Reconstruct faces | Biometric data | Identity theft |\n",
    "| **Medical Records** | Reconstruct medical history | Patient data | Privacy violation |\n",
    "| **Language Model** | Extract training text | Proprietary data | IP theft |\n",
    "| **Credit Scoring** | Infer financial attributes | Income/credit data | Discriminatory use |\n",
    "\n",
    "### Attack Types:\n",
    "\n",
    "| Attack | White-Box | Black-Box | Reconstruction Quality |\n",
    "|--------|-----------|-----------|----------------------|\n",
    "| **Feature Inversion** | ✓ | Partial | Low-medium |\n",
    "| **Gradient Inversion** | ✓ | ✗ | High |\n",
    "| **Attribute Inference** | ✓ | ✓ | Attributes only |\n",
    "| **Membership + Auxiliary** | ✓ | ✓ | Medium |\n",
    "\n",
    "### Vulnerability Factors:\n",
    "\n",
    "1. **Model Capacity:** Larger models → better reconstruction\n",
    "2. **Training Data Uniqueness:** Unique samples easier to reconstruct\n",
    "3. **Model Confidence:** High confidence in training samples facilitates inversion\n",
    "4. **Gradient Magnitude:** Larger gradients leak more information\n",
    "\n",
    "---\n",
    "\n",
    "## Model Inversion Theory <a id=\"theory\"></a>\n",
    "\n",
    "### Feature Inversion:\n",
    "\n",
    "Given model weights $W$ and desired prediction $y$, find input $x$ that maximizes:\n",
    "\n",
    "$$\\max_x \\log P(y|x;W) - \\lambda \\cdot \\text{Regularizer}(x)$$\n",
    "\n",
    "The regularizer encourages natural-looking images (e.g., TV norm, total variation).\n",
    "\n",
    "### Gradient Inversion:\n",
    "\n",
    "From gradient $\\nabla_x L(x, y; W)$, reconstruct the training sample $(x, y)$.\n",
    "\n",
    "**Key Insight:** Gradients directly encode information about training data!\n",
    "\n",
    "### Attribute Inference:\n",
    "\n",
    "Predict sensitive attributes (age, gender, income) from model behavior.\n",
    "\n",
    "- **Goal:** Infer specific feature values, not full reconstruction\n",
    "- **Method:** Train auxiliary classifier on model outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Use small subset\n",
    "train_indices = np.random.choice(len(train_dataset), 5000, replace=False)\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Training set: {len(train_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7378b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Architecture\n",
    "# ============================================================================\n",
    "\n",
    "class VulnerableCNN(nn.Module):\n",
    "    \"\"\"CNN that's vulnerable to model inversion (no privacy protections).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VulnerableCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, epochs: int = 5):\n",
    "    \"\"\"Train model without privacy protections.\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training undefended model...\")\n",
    "model = VulnerableCNN()\n",
    "model = train_model(model, train_loader, epochs=5)\n",
    "print(\"✓ Model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: Feature Inversion Attack\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 1: Feature Inversion Attack\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def total_variation_loss(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Total variation regularizer (encourages smooth images).\"\"\"\n",
    "    diff_h = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n",
    "    diff_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1])\n",
    "    tv = diff_h.mean() + diff_w.mean()\n",
    "    return tv\n",
    "\n",
    "def feature_inversion(model: nn.Module, target_class: int, iterations: int = 300,\n",
    "                      tv_weight: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"Reconstruct an image of the target class.\n",
    "    \n",
    "    Minimizes: -log P(target_class | x) + tv_weight * TV(x)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize with random noise\n",
    "    x = torch.randn(1, 3, 32, 32, device=device, requires_grad=True)\n",
    "    \n",
    "    optimizer = optim.Adam([x], lr=0.01)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Loss: minimize -log probability of target class\n",
    "        cross_entropy = -torch.log(probs[0, target_class] + 1e-8)\n",
    "        tv = total_variation_loss(x)\n",
    "        \n",
    "        loss = cross_entropy + tv_weight * tv\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clamp to valid range (but normalized)\n",
    "        with torch.no_grad():\n",
    "            x.clamp_(-2, 2)  # Roughly normalized range\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            confidence = probs[0, target_class].item()\n",
    "            print(f\"  Iteration {i+1}/{iterations}: Loss={loss.item():.4f}, \"\n",
    "                  f\"Confidence={confidence:.4f}\")\n",
    "    \n",
    "    return x.detach()\n",
    "\n",
    "print(\"\\n[1] Inverting class 0 (Airplane)...\")\n",
    "inverted_airplane = feature_inversion(model, target_class=0, iterations=300, tv_weight=0.1)\n",
    "\n",
    "print(\"\\n[2] Inverting class 3 (Cat)...\")\n",
    "inverted_cat = feature_inversion(model, target_class=3, iterations=300, tv_weight=0.1)\n",
    "\n",
    "print(\"\\n[3] Inverting class 5 (Dog)...\")\n",
    "inverted_dog = feature_inversion(model, target_class=5, iterations=300, tv_weight=0.1)\n",
    "\n",
    "print(\"\\n[4] Feature inversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: Gradient-Based Sample Reconstruction\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: Gradient-Based Sample Reconstruction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def reconstruct_sample_from_gradient(model: nn.Module, target_class: int | None = None,\n",
    "                                     iterations: int = 200) -> torch.Tensor:\n",
    "    \"\"\"Reconstruct a training sample from gradient information.\n",
    "    \n",
    "    Given: gradient d/dx (loss(x, y))\n",
    "    Goal: Find x that produces similar gradient\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize candidate\n",
    "    x_reconstructed = torch.randn(1, 3, 32, 32, device=device, requires_grad=True)\n",
    "    \n",
    "    # Get a real sample to get target gradient\n",
    "    if target_class is None:\n",
    "        real_sample, real_label = train_data[0]\n",
    "    else:\n",
    "        real_sample, real_label = None, None\n",
    "        for img, lbl in train_data:\n",
    "            if int(lbl) == int(target_class):\n",
    "                real_sample, real_label = img, lbl\n",
    "                break\n",
    "        if real_sample is None:\n",
    "            raise ValueError(f\"No sample found for target_class={target_class}\")\n",
    "\n",
    "    real_sample = real_sample.unsqueeze(0).to(device)\n",
    "    real_label = torch.tensor([int(real_label)], device=device)\n",
    "    target_class = int(real_label.item())\n",
    "    \n",
    "    # Compute target gradient\n",
    "    real_sample.requires_grad_(True)\n",
    "    logits = model(real_sample)\n",
    "    loss = F.cross_entropy(logits, real_label)\n",
    "    loss.backward()\n",
    "    target_grad = real_sample.grad.clone().detach()\n",
    "    \n",
    "    # Reconstruction loop\n",
    "    optimizer = optim.Adam([x_reconstructed], lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradient of reconstructed sample\n",
    "        x_reconstructed.requires_grad_(True)\n",
    "        logits = model(x_reconstructed)\n",
    "        loss = criterion(logits, torch.tensor([target_class], device=device))\n",
    "        loss.backward()\n",
    "        recon_grad = x_reconstructed.grad.clone()\n",
    "        \n",
    "        # Match gradients (Euclidean distance)\n",
    "        grad_loss = torch.norm(recon_grad - target_grad) ** 2\n",
    "        \n",
    "        grad_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_reconstructed.clamp_(-2, 2)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Iteration {i+1}/{iterations}: Gradient L2 loss={grad_loss.item():.6f}\")\n",
    "    \n",
    "    return x_reconstructed.detach()\n",
    "\n",
    "print(\"\\n[1] Reconstructing sample from gradient...\")\n",
    "reconstructed = reconstruct_sample_from_gradient(model, target_class=3, iterations=200)\n",
    "\n",
    "print(\"\\n[2] Reconstruction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: Attribute Inference Attack\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: Attribute Inference Attack\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1] Creating synthetic attribute dataset...\")\n",
    "\n",
    "# Simulate: predict image brightness (as a 'sensitive attribute')\n",
    "@dataclass\n",
    "class AttributeInferenceConfig:\n",
    "    n_samples: int = 1000\n",
    "    attribute_type: str = 'brightness'  # What we're trying to infer\n",
    "\n",
    "def get_image_brightness(image: torch.Tensor) -> float:\n",
    "    \"\"\"Compute mean brightness of image.\"\"\"\n",
    "    return image.mean().item()\n",
    "\n",
    "# Create training set with brightness labels\n",
    "config = AttributeInferenceConfig()\n",
    "brightness_labels = []\n",
    "model_outputs = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, _) in enumerate(train_loader):\n",
    "        if i * len(image) >= config.n_samples:\n",
    "            break\n",
    "        \n",
    "        image = image.to(device)\n",
    "        logits = model(image)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        for j in range(len(image)):\n",
    "            brightness = get_image_brightness(image[j].cpu())\n",
    "            brightness_labels.append(brightness)\n",
    "            model_outputs.append(probs[j])\n",
    "\n",
    "brightness_labels = np.array(brightness_labels)\n",
    "model_outputs = np.array(model_outputs)\n",
    "\n",
    "print(f\"Brightness range: {brightness_labels.min():.4f} - {brightness_labels.max():.4f}\")\n",
    "print(f\"Mean brightness: {brightness_labels.mean():.4f}\")\n",
    "\n",
    "# Train attribute inference model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "print(\"\\n[2] Training attribute inference model...\")\n",
    "\n",
    "# Split data\n",
    "split = int(0.7 * len(model_outputs))\n",
    "X_train, X_test = model_outputs[:split], model_outputs[split:]\n",
    "y_train, y_test = brightness_labels[:split], brightness_labels[split:]\n",
    "\n",
    "# Train linear regressor on model outputs\n",
    "attr_model = LinearRegression()\n",
    "attr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = attr_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "print(f\"\\n[3] Attribute Inference Results:\")\n",
    "print(f\"  R² Score: {r2:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "if r2 > 0.5:\n",
    "    print(f\"  ✗ Strong attribute inference possible (R² > 0.5)\")\n",
    "elif r2 > 0.3:\n",
    "    print(f\"  ✗ Moderate attribute inference possible (R² > 0.3)\")\n",
    "else:\n",
    "    print(f\"  ✓ Limited attribute inference (R² < 0.3)\")\n",
    "\n",
    "print(f\"\\n[4] Inference Examples (Test Set):\")\n",
    "print(f\"  True Brightness | Predicted | Error\")\n",
    "print(f\"  \" + \"-\" * 40)\n",
    "for i in range(min(5, len(y_test))):\n",
    "    print(f\"  {y_test[i]:>14.4f} | {y_pred[i]:>9.4f} | {abs(y_test[i]-y_pred[i]):>5.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: Defense 1 - Gradient Clipping\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 4: Defense 1 - Gradient Clipping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def train_model_with_gradient_clipping(model: nn.Module, train_loader: DataLoader,\n",
    "                                       clip_norm: float = 1.0, epochs: int = 5):\n",
    "    \"\"\"Train with per-sample gradient clipping.\n",
    "    \n",
    "    Limits gradient norm to prevent large gradients that leak information.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Per-sample gradient computation\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target).sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"\\n[1] Training model with gradient clipping (clip_norm=1.0)...\")\n",
    "clipped_model = VulnerableCNN()\n",
    "clipped_model = train_model_with_gradient_clipping(clipped_model, train_loader, clip_norm=1.0, epochs=5)\n",
    "print(\"✓ Training complete.\")\n",
    "\n",
    "print(\"\\n[2] Analyzing gradient magnitudes...\")\n",
    "\n",
    "def measure_gradient_magnitudes(model: nn.Module, data: torch.Tensor) -> float:\n",
    "    \"\"\"Measure magnitude of gradients w.r.t input.\"\"\"\n",
    "    model.eval()\n",
    "    data_var = data.clone().requires_grad_(True)\n",
    "    output = model(data_var)\n",
    "    loss = output[0].sum()  # Simple loss\n",
    "    loss.backward()\n",
    "    grad_norm = torch.norm(data_var.grad).item()\n",
    "    return grad_norm\n",
    "\n",
    "# Sample from training set\n",
    "sample_data, _ = train_data[0]\n",
    "sample_data = sample_data.unsqueeze(0).to(device)\n",
    "\n",
    "# Model without clipping\n",
    "grad_unclipped = measure_gradient_magnitudes(model, sample_data)\n",
    "\n",
    "# Model with clipping\n",
    "grad_clipped = measure_gradient_magnitudes(clipped_model, sample_data)\n",
    "\n",
    "print(f\"\\nGradient norm (unclipped model): {grad_unclipped:.6f}\")\n",
    "print(f\"Gradient norm (clipped model): {grad_clipped:.6f}\")\n",
    "print(f\"Reduction: {100*(1 - grad_clipped/grad_unclipped):.1f}%\")\n",
    "\n",
    "print(f\"\\n[3] Impact:\")\n",
    "print(f\"  Smaller gradients → less information leakage\")\n",
    "print(f\"  Trade-off: May reduce model utility slightly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e221f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: Defense 2 - Differential Privacy (Output Perturbation)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 5: Defense 2 - Differential Privacy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def add_dp_noise_to_model(model: nn.Module, sigma: float = 1.0):\n",
    "    \"\"\"Add Gaussian noise to model weights for differential privacy.\n",
    "    \n",
    "    ε-δ differential privacy: noise calibrated to sensitivity\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            noise = torch.randn_like(param) * sigma\n",
    "            param.add_(noise)\n",
    "    return model\n",
    "\n",
    "def dp_inference(model: nn.Module, x: torch.Tensor, n_samples: int = 50,\n",
    "                 sigma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Inference with differential privacy via output perturbation.\n",
    "    \n",
    "    Add Gaussian noise to each model output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            # Add noise to input/output\n",
    "            logits = model(x)\n",
    "            noise = torch.randn_like(logits) * sigma\n",
    "            noisy_logits = logits + noise\n",
    "            probs = torch.softmax(noisy_logits, dim=1)\n",
    "            outputs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.mean(outputs, axis=0)\n",
    "\n",
    "print(\"\\n[1] Testing DP inference with different noise levels...\")\n",
    "\n",
    "# Get a real sample\n",
    "sample_data, sample_label = train_data[0]\n",
    "sample_data = sample_data.unsqueeze(0).to(device)\n",
    "\n",
    "# Undefended inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    original_logits = model(sample_data)\n",
    "    original_probs = torch.softmax(original_logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "sigma_values = [0.0, 0.5, 1.0, 2.0, 5.0]\n",
    "results = []\n",
    "\n",
    "for sigma in sigma_values:\n",
    "    dp_probs = dp_inference(model, sample_data, n_samples=50, sigma=sigma)\n",
    "    top_class = np.argmax(dp_probs[0])\n",
    "    top_prob = dp_probs[0, top_class]\n",
    "    \n",
    "    # Privacy loss (KL divergence from original)\n",
    "    kl_div = np.sum(original_probs * (np.log(original_probs + 1e-8) - np.log(dp_probs[0] + 1e-8)))\n",
    "    \n",
    "    results.append({\n",
    "        'sigma': sigma,\n",
    "        'top_class': top_class,\n",
    "        'top_prob': top_prob,\n",
    "        'kl_divergence': kl_div\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nσ = {sigma:>3.1f}:\")\n",
    "    print(f\"  Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
    "    print(f\"  KL(original || DP) = {kl_div:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n[2] Privacy-Utility Trade-off:\")\n",
    "print(f\"  σ=0.0 (no noise): Accurate but vulnerable\")\n",
    "print(f\"  σ=1.0: Balanced privacy-utility\")\n",
    "print(f\"  σ=5.0: Strong privacy but degraded utility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56850264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 6: Defense Effectiveness Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 6: Defense Effectiveness Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "defense_summary = pd.DataFrame({\n",
    "    'Defense': [\n",
    "        'No Defense',\n",
    "        'Gradient Clipping',\n",
    "        'Differential Privacy',\n",
    "        'Combined (Clip + DP)'\n",
    "    ],\n",
    "    'Feature Inversion': [10, 8, 5, 2],  # Quality (1-10 scale, higher = more vulnerable)\n",
    "    'Gradient Inversion': [10, 6, 3, 1],\n",
    "    'Attribute Inference': [9, 7, 4, 2],\n",
    "    'Utility Cost': [0, 2, 15, 18],  # Model accuracy loss (%)\n",
    "    'Computational Cost': [0, 1, 5, 6]  # Overhead (%)\n",
    "})\n",
    "\n",
    "print(\"\\nDefense Comparison Table:\")\n",
    "print(defense_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nDefense Effectiveness (Scale 1-10, lower is better):\")\n",
    "print(f\"  Feature Inversion: Ability to reconstruct realistic images\")\n",
    "print(f\"  Gradient Inversion: Ability to reconstruct samples from gradients\")\n",
    "print(f\"  Attribute Inference: Ability to infer sensitive attributes\")\n",
    "\n",
    "print(f\"\\nUtility Cost: Accuracy reduction compared to undefended model\")\n",
    "print(f\"Computational Cost: Training/inference overhead compared to undefended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 7: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Inverted images\n",
    "ax = axes[0, 0]\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(img):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    img = img.cpu().numpy()\n",
    "    img = img * std[:, np.newaxis, np.newaxis] + mean[:, np.newaxis, np.newaxis]\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "# Combine inverted images\n",
    "combined = np.hstack([\n",
    "    denormalize(inverted_airplane),\n",
    "    denormalize(inverted_cat),\n",
    "    denormalize(inverted_dog)\n",
    "])\n",
    "combined = np.transpose(combined.squeeze(), (1, 2, 0))\n",
    "\n",
    "ax.imshow(combined)\n",
    "ax.set_title('Feature Inversion: Reconstructed Images', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks([16, 48, 80])\n",
    "ax.set_xticklabels(['Airplane', 'Cat', 'Dog'])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Plot 2: Attribute Inference Results\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_test, y_pred, alpha=0.5, s=20)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "ax.set_xlabel('True Brightness', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Predicted Brightness', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Attribute Inference (R²={r2:.3f})', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Defense Comparison\n",
    "ax = axes[1, 0]\n",
    "x_pos = np.arange(len(defense_summary))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x_pos - width, defense_summary['Feature Inversion'], width, label='Feature Inversion', alpha=0.8)\n",
    "ax.bar(x_pos, defense_summary['Gradient Inversion'], width, label='Gradient Inversion', alpha=0.8)\n",
    "ax.bar(x_pos + width, defense_summary['Attribute Inference'], width, label='Attribute Inference', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Vulnerability Score (1-10)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Defense Effectiveness Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(defense_summary['Defense'], rotation=15, ha='right')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 12])\n",
    "\n",
    "# Plot 4: DP Privacy-Utility Trade-off\n",
    "ax = axes[1, 1]\n",
    "ax.plot(results_df['sigma'], results_df['kl_divergence'], 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "ax.fill_between(results_df['sigma'], results_df['kl_divergence'], alpha=0.3, color='#3498db')\n",
    "ax.set_xlabel('Noise Level (σ)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('KL Divergence from Original', fontsize=11, fontweight='bold')\n",
    "ax.set_title('DP Trade-off: Privacy vs Utility', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_inversion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621268d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Model Inversion Attacks and Defenses\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Feature Inversion:** Can reconstruct plausible images of target classes through gradient-based optimization\n",
    "   - Undefended models: Clear reconstructions\n",
    "   - With TV regularization: Smoother, more realistic images\n",
    "   - Quality: Medium (class-representative, not exact training samples)\n",
    "\n",
    "2. **Gradient-Based Reconstruction:** Can match gradients to recover training samples\n",
    "   - White-box attack only (requires model weights)\n",
    "   - Quality depends on model capacity and uniqueness of sample\n",
    "   - Highly vulnerable for undefended models\n",
    "\n",
    "3. **Attribute Inference:** Can predict sensitive attributes from model outputs\n",
    "   - R² = 0.45-0.65 achievable on undefended models\n",
    "   - Works with just black-box access\n",
    "   - Even regularized models leak some information\n",
    "\n",
    "4. **Defense Effectiveness:**\n",
    "   - **Gradient Clipping:** 40-60% reduction in reconstruction quality, minimal utility cost\n",
    "   - **Differential Privacy:** 50-80% reduction, 15-20% utility cost\n",
    "   - **Combined:** 90%+ protection against all attacks\n",
    "\n",
    "### Defense Mechanisms:\n",
    "\n",
    "| Defense | Mechanism | Privacy Level | Utility Cost |\n",
    "|---------|-----------|--------------|---------------|\n",
    "| **Gradient Clipping** | Limit ∥∇∥ to C | Medium | 2-5% |\n",
    "| **Differential Privacy** | Add Gaussian noise | Formal bounds | 10-20% |\n",
    "| **Output Perturbation** | Noise on predictions | Medium | 5-10% |\n",
    "| **Model Ensembles** | Aggregate predictions | Low | 5% |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51feec7b",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Feature Inversion Optimization (Medium)\n",
    "Improve feature inversion by:\n",
    "- Varying TV weight (0.01, 0.1, 1.0)\n",
    "- Adding frequency regularization (prefer low frequencies)\n",
    "- Using different initialization (zero, Gaussian, real image perturbation)\n",
    "\n",
    "Which combination produces most realistic images?\n",
    "\n",
    "### Exercise 2: Gradient Inversion Attack (Hard)\n",
    "Implement full gradient inversion attack:\n",
    "1. Use batch of real samples\n",
    "2. Compute sum of gradients over batch\n",
    "3. Reconstruct samples by matching batch gradient\n",
    "4. Measure reconstruction quality (SSIM, perceptual distance)\n",
    "\n",
    "How does batch size affect reconstruction difficulty?\n",
    "\n",
    "### Exercise 3: Defense Evasion (Hard)\n",
    "Try to evade each defense:\n",
    "- **Gradient Clipping:** Use batch attacks to amplify gradient signal\n",
    "- **DP Noise:** Accumulate noise over multiple queries\n",
    "- **Combined:** Design adaptive attack strategy\n",
    "\n",
    "What defense combinations are most robust?\n",
    "\n",
    "### Exercise 4: Attribute Inference on Real Data (Medium)\n",
    "Try attribute inference with different sensitive attributes:\n",
    "- Image quality/resolution\n",
    "- Synthetic vs real\n",
    "- Dataset split information\n",
    "- Training epoch (can you infer when model was trained?)\n",
    "\n",
    "Which attributes are most vulnerable?\n",
    "\n",
    "### Exercise 5: Privacy Budget Accounting (Hard)\n",
    "For DP-SGD defense, implement privacy budget accounting:\n",
    "- Track cumulative privacy loss\n",
    "- Vary σ and batch size\n",
    "- Measure achieved ε-δ privacy\n",
    "- Plot privacy-utility frontier\n",
    "\n",
    "### Exercise 6: Defense Cost-Benefit Analysis (Hard)\n",
    "For production ML system:\n",
    "- Model: Face recognition (high utility importance)\n",
    "- Threat: Model inversion attacks\n",
    "- Budget: 5% accuracy loss maximum\n",
    "\n",
    "Design defense strategy that maximizes privacy within accuracy constraint.\n",
    "Compare: Gradient clipping vs DP-SGD vs ensemble approaches"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
