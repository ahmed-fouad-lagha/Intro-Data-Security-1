{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae3d3e3",
   "metadata": {},
   "source": [
    "# **Lab 2a: Evasion Attacks (Adversarial Examples)**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr.  \n",
    "**Module 2:** Input Data Manipulation  \n",
    "**Estimated Time:** 90–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Generate adversarial examples using FGSM and PGD\n",
    "- Measure model robustness under $\\ell_\\infty$ perturbations\n",
    "- Visualize perturbations and their impact on predictions\n",
    "- Compare clean vs. adversarial accuracy\n",
    "- Discuss the robustness–accuracy tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "1. Setup & Imports  \n",
    "2. Load Model & Dataset  \n",
    "3. Baseline Evaluation  \n",
    "4. FGSM Attack  \n",
    "5. PGD Attack  \n",
    "6. Robustness Evaluation  \n",
    "7. Visual Analysis  \n",
    "8. Exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111b8bb",
   "metadata": {},
   "source": [
    "## **1. Setup & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies\n",
    "# !pip install torch torchvision matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85042f4a",
   "metadata": {},
   "source": [
    "## **2. Load Model & Dataset**\n",
    "\n",
    "We will reuse the MNIST model trained in Lab 1a. If you don’t have the checkpoint, train a quick baseline model in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Optional train set for fallback training\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define a simple CNN (use same as Lab 1a)\n",
    "class StandardCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = StandardCNN().to(device)\n",
    "\n",
    "# Try loading weights from Lab 1a\n",
    "model_path = '../module_01_foundations/standard_mnist_cnn.pth'\n",
    "loaded = False\n",
    "try:\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded = True\n",
    "    print(\"Loaded model from Lab 1a\")\n",
    "except Exception as e:\n",
    "    print(\"Model checkpoint not found. Training a quick baseline below.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Normalized clamp bounds for MNIST\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "min_val = (0.0 - mnist_mean) / mnist_std\n",
    "max_val = (1.0 - mnist_mean) / mnist_std\n",
    "print(f\"Normalized clamp range: [{min_val:.2f}, {max_val:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d40cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional fallback training if checkpoint is missing\n",
    "if not loaded:\n",
    "    print(\"Training a quick baseline model (2 epochs, limited batches)...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    max_batches = 100\n",
    "    for epoch in range(2):\n",
    "        running_loss = 0.0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/2: Loss={running_loss / max_batches:.4f}\")\n",
    "    model.eval()\n",
    "    print(\"Baseline training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5eb03a",
   "metadata": {},
   "source": [
    "## **3. Baseline Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "clean_acc = evaluate(model, test_loader)\n",
    "print(f\"Clean accuracy: {clean_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bfff0",
   "metadata": {},
   "source": [
    "## **4. FGSM Attack**\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) crafts adversarial examples by taking a single step in the direction of the input gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053de4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, x, y, epsilon=0.3):\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    x.requires_grad = True\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = nn.CrossEntropyLoss()(logits, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    perturbation = epsilon * x.grad.sign()\n",
    "    x_adv = x + perturbation\n",
    "    x_adv = torch.clamp(x_adv, min_val, max_val)\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7e40c",
   "metadata": {},
   "source": [
    "## **5. PGD Attack**\n",
    "\n",
    "Projected Gradient Descent (PGD) iterates FGSM steps and projects back into the $\\epsilon$-ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, x, y, epsilon=0.3, alpha=0.01, steps=40):\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    x_adv = x + 0.001 * torch.randn_like(x)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad = True\n",
    "        logits = model(x_adv)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.sign()\n",
    "\n",
    "        x_adv = x_adv + alpha * grad\n",
    "        perturbation = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "        x_adv = torch.clamp(x + perturbation, min_val, max_val).detach()\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762898a",
   "metadata": {},
   "source": [
    "## **6. Robustness Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b819d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_accuracy(model, loader, attack_fn, **kwargs):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_adv = attack_fn(model, x, y, **kwargs)\n",
    "        logits = model(x_adv)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "fgsm_acc = adversarial_accuracy(model, test_loader, fgsm_attack, epsilon=0.3)\n",
    "pgd_acc = adversarial_accuracy(model, test_loader, pgd_attack, epsilon=0.3, alpha=0.01, steps=20)\n",
    "\n",
    "print(f\"FGSM accuracy (eps=0.3): {fgsm_acc * 100:.2f}%\")\n",
    "print(f\"PGD accuracy (eps=0.3):  {pgd_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab384e",
   "metadata": {},
   "source": [
    "## **7. Visual Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial examples\n",
    "x, y = next(iter(test_loader))\n",
    "\n",
    "x_adv_fgsm = fgsm_attack(model, x[:5], y[:5], epsilon=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow((x[i].squeeze() * 0.3081 + 0.1307).cpu(), cmap='gray')\n",
    "    axes[0, i].set_title(f\"Clean: {y[i].item()}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow((x_adv_fgsm[i].squeeze() * 0.3081 + 0.1307).cpu(), cmap='gray')\n",
    "    axes[1, i].set_title(\"FGSM\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bdb52",
   "metadata": {},
   "source": [
    "## **8. Exercises**\n",
    "\n",
    "1. Run FGSM with $\\epsilon \\in \\{0.05, 0.1, 0.2, 0.3\\}$ and plot accuracy vs. $\\epsilon$.  \n",
    "2. Increase PGD steps to 50 and analyze how robustness changes.  \n",
    "3. Compare $\\ell_\\infty$ vs. $\\ell_2$ constraints (bonus).  \n",
    "4. Discuss why PGD is generally stronger than FGSM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
