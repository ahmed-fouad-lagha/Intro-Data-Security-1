{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34be77aa",
   "metadata": {},
   "source": [
    "# **Lab 1b: Threat Modeling & Attack Taxonomy in AI Security**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr. (Master's Level)  \n",
    "**Module 1:** Foundations  \n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand** the fundamental security properties in AI/ML systems (CIA triad)\n",
    "2. **Classify** attacks based on timing (training vs. test-time) and objectives\n",
    "3. **Analyze** threat models and adversarial capabilities\n",
    "4. **Formalize** security objectives for ML systems\n",
    "5. **Apply** attack taxonomy to real-world scenarios\n",
    "6. **Design** appropriate defenses based on threat analysis\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction to AI Security](#intro)\n",
    "2. [The CIA Triad in Machine Learning](#cia)\n",
    "3. [Attack Taxonomy Framework](#taxonomy)\n",
    "4. [Threat Modeling Methodology](#threat-modeling)\n",
    "5. [Attack Surface Analysis](#attack-surface)\n",
    "6. [Real-World Case Studies](#case-studies)\n",
    "7. [Defense Strategy Framework](#defense)\n",
    "8. [Exercises](#exercises)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636c7db",
   "metadata": {},
   "source": [
    "## **1. Introduction to AI Security** <a name=\"intro\"></a>\n",
    "\n",
    "### **Why is AI Security Different?**\n",
    "\n",
    "Traditional cybersecurity focuses on protecting systems from unauthorized access and code execution. **AI Security** adds unique challenges:\n",
    "\n",
    "| Traditional Security | AI Security |\n",
    "|---------------------|-------------|\n",
    "| Binary outcomes (works/fails) | Probabilistic outputs |\n",
    "| Code-based vulnerabilities | Data-based vulnerabilities |\n",
    "| Known attack patterns | Adaptive adversaries |\n",
    "| Deterministic behavior | Statistical learning |\n",
    "| Code inspection possible | Model internals opaque |\n",
    "\n",
    "### **The Machine Learning Pipeline Attack Surface**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Data      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Training ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Model  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Deployment ‚îÇ\n",
    "‚îÇ Collection  ‚îÇ     ‚îÇ Process  ‚îÇ     ‚îÇ         ‚îÇ     ‚îÇ & Inference‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚ñ≤                  ‚ñ≤                ‚ñ≤                 ‚ñ≤\n",
    "      ‚îÇ                  ‚îÇ                ‚îÇ                 ‚îÇ\n",
    "   Poisoning         Backdoors        Model             Evasion\n",
    "   Attacks           Trojans          Theft             Attacks\n",
    "```\n",
    "\n",
    "Each stage presents unique attack opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add31633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869fc1d",
   "metadata": {},
   "source": [
    "## **2. The CIA Triad in Machine Learning** <a name=\"cia\"></a>\n",
    "\n",
    "The classic **CIA Triad** applies to ML systems with specific interpretations:\n",
    "\n",
    "### **2.1. Confidentiality**\n",
    "**Definition:** Protecting sensitive information from unauthorized access.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Training data privacy:** Preventing leakage of training samples\n",
    "- **Model privacy:** Protecting model parameters and architecture\n",
    "- **Inference privacy:** Securing user queries and predictions\n",
    "\n",
    "**Example Attacks:**\n",
    "- Model Inversion: Reconstruct training data from model\n",
    "- Membership Inference: Determine if data was in training set\n",
    "- Model Extraction: Steal model functionality via queries\n",
    "\n",
    "### **2.2. Integrity**\n",
    "**Definition:** Ensuring data and model behavior are not maliciously altered.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Data integrity:** Training data is not poisoned\n",
    "- **Model integrity:** Model behaves as intended\n",
    "- **Prediction integrity:** Outputs are trustworthy\n",
    "\n",
    "**Example Attacks:**\n",
    "- Data Poisoning: Inject malicious samples into training data\n",
    "- Backdoor Attacks: Embed hidden triggers in model\n",
    "- Model Poisoning: Corrupt model parameters or architecture\n",
    "\n",
    "### **2.3. Availability**\n",
    "**Definition:** Ensuring the system remains accessible and functional.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Service availability:** Model can respond to queries\n",
    "- **Performance availability:** Acceptable inference latency\n",
    "- **Resource availability:** Computational resources not exhausted\n",
    "\n",
    "**Example Attacks:**\n",
    "- Sponge Attacks: Force high computational cost at inference\n",
    "- Denial of Service: Overwhelm model with queries\n",
    "- Resource Exhaustion: Deplete GPU/CPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CIA Triad in ML\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Data for visualization\n",
    "cia_data = {\n",
    "    'Confidentiality': {\n",
    "        'attacks': ['Model Inversion', 'Membership Inference', 'Model Extraction'],\n",
    "        'colors': ['#e74c3c', '#c0392b', '#a93226']\n",
    "    },\n",
    "    'Integrity': {\n",
    "        'attacks': ['Data Poisoning', 'Backdoor Attack', 'Model Corruption'],\n",
    "        'colors': ['#3498db', '#2980b9', '#21618c']\n",
    "    },\n",
    "    'Availability': {\n",
    "        'attacks': ['Sponge Attack', 'DoS', 'Resource Exhaustion'],\n",
    "        'colors': ['#2ecc71', '#27ae60', '#1e8449']\n",
    "    }\n",
    "}\n",
    "\n",
    "for idx, (principle, data) in enumerate(cia_data.items()):\n",
    "    ax = axes[idx]\n",
    "    y_pos = np.arange(len(data['attacks']))\n",
    "    ax.barh(y_pos, [3, 2.5, 2], color=data['colors'])\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(data['attacks'])\n",
    "    ax.set_xlabel('Severity', fontweight='bold')\n",
    "    ax.set_title(f'{principle}\\nAttacks', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim([0, 3.5])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cia_triad_ml.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì CIA Triad visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5aab0",
   "metadata": {},
   "source": [
    "## **3. Attack Taxonomy Framework** <a name=\"taxonomy\"></a>\n",
    "\n",
    "We classify attacks along multiple dimensions:\n",
    "\n",
    "### **Dimension 1: Attack Timing**\n",
    "\n",
    "#### **Training-Time Attacks (Poisoning)**\n",
    "- Occur during model training\n",
    "- Attacker manipulates training data or process\n",
    "- Effects persist in deployed model\n",
    "- Examples: Data poisoning, backdoor injection\n",
    "\n",
    "#### **Test-Time Attacks (Evasion)**\n",
    "- Occur during inference/deployment\n",
    "- Attacker manipulates input to deployed model\n",
    "- No modification to model itself\n",
    "- Examples: Adversarial examples, adversarial prompts\n",
    "\n",
    "### **Dimension 2: Security Objective Violated**\n",
    "\n",
    "| Attack Category | CIA Principle | Timing | Goal |\n",
    "|----------------|---------------|--------|------|\n",
    "| **Evasion** | Integrity | Test-time | Misclassify specific inputs |\n",
    "| **Poisoning** | Integrity | Training-time | Corrupt model behavior |\n",
    "| **Privacy** | Confidentiality | Any | Extract sensitive info |\n",
    "| **Sponge** | Availability | Test-time | Degrade performance |\n",
    "| **Model Extraction** | Confidentiality | Test-time | Steal model functionality |\n",
    "\n",
    "### **Dimension 3: Attacker Knowledge**\n",
    "\n",
    "#### **White-Box Attacks**\n",
    "- Full knowledge of model architecture\n",
    "- Access to model parameters\n",
    "- Can compute gradients\n",
    "- Most powerful attacks\n",
    "\n",
    "#### **Black-Box Attacks**\n",
    "- No access to model internals\n",
    "- Only query-response access\n",
    "- Must infer model behavior\n",
    "- More realistic threat model\n",
    "\n",
    "#### **Gray-Box Attacks**\n",
    "- Partial knowledge (e.g., architecture but not weights)\n",
    "- Limited access to internals\n",
    "- Between white-box and black-box\n",
    "\n",
    "### **Dimension 4: Attack Specificity**\n",
    "\n",
    "#### **Targeted Attacks**\n",
    "- Goal: Cause specific misclassification\n",
    "- Example: Make \"stop sign\" classified as \"speed limit\"\n",
    "- Harder to achieve\n",
    "- More dangerous in practice\n",
    "\n",
    "#### **Untargeted Attacks**\n",
    "- Goal: Cause any misclassification\n",
    "- Example: Make model fail on any input\n",
    "- Easier to achieve\n",
    "- Useful for measuring robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb971a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive attack taxonomy table\n",
    "attack_taxonomy = pd.DataFrame({\n",
    "    'Attack Type': [\n",
    "        'FGSM/PGD',\n",
    "        'Data Poisoning',\n",
    "        'Backdoor Attack',\n",
    "        'Model Inversion',\n",
    "        'Membership Inference',\n",
    "        'Sponge Attack',\n",
    "        'Model Extraction',\n",
    "        'Adversarial Prompts'\n",
    "    ],\n",
    "    'Timing': [\n",
    "        'Test-time',\n",
    "        'Training-time',\n",
    "        'Training-time',\n",
    "        'Test-time',\n",
    "        'Test-time',\n",
    "        'Test-time',\n",
    "        'Test-time',\n",
    "        'Test-time'\n",
    "    ],\n",
    "    'CIA Violation': [\n",
    "        'Integrity',\n",
    "        'Integrity',\n",
    "        'Integrity',\n",
    "        'Confidentiality',\n",
    "        'Confidentiality',\n",
    "        'Availability',\n",
    "        'Confidentiality',\n",
    "        'Integrity'\n",
    "    ],\n",
    "    'Attacker Access': [\n",
    "        'White/Black-box',\n",
    "        'Data Access',\n",
    "        'Data Access',\n",
    "        'White/Black-box',\n",
    "        'Black-box',\n",
    "        'Black-box',\n",
    "        'Black-box',\n",
    "        'Black-box'\n",
    "    ],\n",
    "    'Detectability': [\n",
    "        'Low',\n",
    "        'Medium',\n",
    "        'Low',\n",
    "        'Low',\n",
    "        'Low',\n",
    "        'High',\n",
    "        'Medium',\n",
    "        'Low'\n",
    "    ],\n",
    "    'Course Coverage': [\n",
    "        'Module 2',\n",
    "        'Module 3',\n",
    "        'Module 4',\n",
    "        'Module 6',\n",
    "        'Module 6',\n",
    "        'Module 5',\n",
    "        'Advanced',\n",
    "        'Module 2'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ATTACK TAXONOMY FOR AI/ML SYSTEMS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(attack_taxonomy.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save for reference\n",
    "attack_taxonomy.to_csv('attack_taxonomy.csv', index=False)\n",
    "print(\"\\n‚úì Attack taxonomy saved to attack_taxonomy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7718b",
   "metadata": {},
   "source": [
    "## **4. Threat Modeling Methodology** <a name=\"threat-modeling\"></a>\n",
    "\n",
    "### **STRIDE Framework for ML**\n",
    "\n",
    "Adapted from Microsoft's STRIDE model:\n",
    "\n",
    "| Threat | ML Interpretation | Example |\n",
    "|--------|------------------|----------|\n",
    "| **S**poofing | Impersonate legitimate data/user | Fake training samples |\n",
    "| **T**ampering | Modify data or model | Poisoning attacks |\n",
    "| **R**epudiation | Deny actions | Untraceable adversarial examples |\n",
    "| **I**nformation Disclosure | Leak sensitive data | Model inversion |\n",
    "| **D**enial of Service | Make system unavailable | Sponge attacks |\n",
    "| **E**levation of Privilege | Gain unauthorized capabilities | Jailbreak LLMs |\n",
    "\n",
    "---\n",
    "\n",
    "### **Threat Modeling Process**\n",
    "\n",
    "**Step 1: Define System Boundaries**\n",
    "```\n",
    "What components are in scope?\n",
    "- Data collection pipeline\n",
    "- Training infrastructure\n",
    "- Deployed model\n",
    "- User interface\n",
    "```\n",
    "\n",
    "**Step 2: Identify Assets**\n",
    "```\n",
    "What needs protection?\n",
    "- Training data (privacy)\n",
    "- Model parameters (IP)\n",
    "- Model predictions (integrity)\n",
    "- System availability\n",
    "```\n",
    "\n",
    "**Step 3: Characterize Adversary**\n",
    "```\n",
    "What can the attacker do?\n",
    "- Access level (white/gray/black-box)\n",
    "- Resources (compute, data, expertise)\n",
    "- Motivation (financial, sabotage, espionage)\n",
    "```\n",
    "\n",
    "**Step 4: Enumerate Attack Paths**\n",
    "```\n",
    "How can attacks be executed?\n",
    "- Training data poisoning\n",
    "- Test-time evasion\n",
    "- Model extraction\n",
    "- Privacy attacks\n",
    "```\n",
    "\n",
    "**Step 5: Prioritize Risks**\n",
    "```\n",
    "Which threats are most critical?\n",
    "Risk = Likelihood √ó Impact\n",
    "```\n",
    "\n",
    "**Step 6: Design Mitigations**\n",
    "```\n",
    "How to defend?\n",
    "- Input validation\n",
    "- Adversarial training\n",
    "- Differential privacy\n",
    "- Monitoring & detection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33583133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Assessment Matrix\n",
    "class ThreatModel:\n",
    "    def __init__(self, system_name):\n",
    "        self.system_name = system_name\n",
    "        self.threats = []\n",
    "    \n",
    "    def add_threat(self, name, likelihood, impact, category):\n",
    "        \"\"\"Add a threat to the model.\n",
    "        \n",
    "        Args:\n",
    "            likelihood: 1-5 (1=rare, 5=almost certain)\n",
    "            impact: 1-5 (1=negligible, 5=catastrophic)\n",
    "        \"\"\"\n",
    "        risk_score = likelihood * impact\n",
    "        self.threats.append({\n",
    "            'name': name,\n",
    "            'likelihood': likelihood,\n",
    "            'impact': impact,\n",
    "            'risk': risk_score,\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    def visualize_risk_matrix(self):\n",
    "        \"\"\"Create risk assessment visualization.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Risk Matrix\n",
    "        df = pd.DataFrame(self.threats)\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = ax1.scatter(df['likelihood'], df['impact'], \n",
    "                            s=df['risk']*30, alpha=0.6, c=df['risk'], \n",
    "                            cmap='RdYlGn_r', edgecolors='black', linewidth=1.5)\n",
    "        \n",
    "        # Add labels\n",
    "        for idx, row in df.iterrows():\n",
    "            ax1.annotate(row['name'], (row['likelihood'], row['impact']),\n",
    "                        fontsize=8, ha='center')\n",
    "        \n",
    "        ax1.set_xlabel('Likelihood', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Impact', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title(f'Risk Matrix: {self.system_name}', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlim([0, 6])\n",
    "        ax1.set_ylim([0, 6])\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add risk zones\n",
    "        ax1.axhline(y=3, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax1.axvline(x=3, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax1.text(1.5, 5.5, 'Low\\nLikelihood\\nHigh Impact', ha='center', fontsize=9)\n",
    "        ax1.text(4.5, 5.5, 'CRITICAL\\nZONE', ha='center', fontsize=11, \n",
    "                fontweight='bold', color='red')\n",
    "        \n",
    "        plt.colorbar(scatter, ax=ax1, label='Risk Score')\n",
    "        \n",
    "        # Bar chart of risks\n",
    "        df_sorted = df.sort_values('risk', ascending=True)\n",
    "        colors = ['green' if r < 10 else 'orange' if r < 15 else 'red' \n",
    "                 for r in df_sorted['risk']]\n",
    "        \n",
    "        ax2.barh(df_sorted['name'], df_sorted['risk'], color=colors, alpha=0.7)\n",
    "        ax2.set_xlabel('Risk Score', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Threat Prioritization', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('threat_model_risk_matrix.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return df.sort_values('risk', ascending=False)\n",
    "\n",
    "# Example: Medical Diagnosis AI System\n",
    "medical_ai = ThreatModel(\"Medical Diagnosis AI System\")\n",
    "\n",
    "# Add threats\n",
    "medical_ai.add_threat('Data Poisoning', likelihood=3, impact=5, category='Integrity')\n",
    "medical_ai.add_threat('Model Inversion', likelihood=4, impact=5, category='Privacy')\n",
    "medical_ai.add_threat('Adversarial Examples', likelihood=4, impact=4, category='Integrity')\n",
    "medical_ai.add_threat('Sponge Attack', likelihood=2, impact=3, category='Availability')\n",
    "medical_ai.add_threat('Model Extraction', likelihood=3, impact=3, category='IP Theft')\n",
    "medical_ai.add_threat('Membership Inference', likelihood=4, impact=4, category='Privacy')\n",
    "\n",
    "# Visualize\n",
    "risk_summary = medical_ai.visualize_risk_matrix()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THREAT PRIORITIZATION (Highest Risk First)\")\n",
    "print(\"=\"*70)\n",
    "print(risk_summary[['name', 'likelihood', 'impact', 'risk', 'category']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906aeef4",
   "metadata": {},
   "source": [
    "## **5. Attack Surface Analysis** <a name=\"attack-surface\"></a>\n",
    "\n",
    "### **ML System Attack Surface Map**\n",
    "\n",
    "Every component in the ML pipeline presents attack opportunities:\n",
    "\n",
    "#### **1. Data Collection & Preparation**\n",
    "**Attack Vectors:**\n",
    "- Inject malicious samples\n",
    "- Corrupt labels\n",
    "- Manipulate feature distributions\n",
    "- Introduce biases\n",
    "\n",
    "**Defenses:**\n",
    "- Data validation\n",
    "- Outlier detection\n",
    "- Statistical testing\n",
    "- Provenance tracking\n",
    "\n",
    "#### **2. Model Training**\n",
    "**Attack Vectors:**\n",
    "- Backdoor insertion\n",
    "- Hyperparameter manipulation\n",
    "- Training process sabotage\n",
    "- Loss function tampering\n",
    "\n",
    "**Defenses:**\n",
    "- Secure training environments\n",
    "- Gradient inspection\n",
    "- Checkpoint verification\n",
    "- Adversarial training\n",
    "\n",
    "#### **3. Model Deployment**\n",
    "**Attack Vectors:**\n",
    "- Model substitution\n",
    "- API exploitation\n",
    "- Query-based attacks\n",
    "- Timing attacks\n",
    "\n",
    "**Defenses:**\n",
    "- Model signing\n",
    "- Rate limiting\n",
    "- Input sanitization\n",
    "- Anomaly detection\n",
    "\n",
    "#### **4. Inference & Serving**\n",
    "**Attack Vectors:**\n",
    "- Adversarial inputs\n",
    "- Privacy extraction\n",
    "- Resource exhaustion\n",
    "- Side-channel attacks\n",
    "\n",
    "**Defenses:**\n",
    "- Input validation\n",
    "- Output filtering\n",
    "- Differential privacy\n",
    "- Resource monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack Surface Visualization\n",
    "def plot_attack_surface():\n",
    "    \"\"\"Visualize attack surface across ML pipeline stages.\"\"\"\n",
    "    \n",
    "    stages = ['Data\\nCollection', 'Feature\\nEngineering', 'Model\\nTraining', \n",
    "              'Validation', 'Deployment', 'Inference']\n",
    "    \n",
    "    # Attack surface score (0-10) for each stage\n",
    "    attack_surface = [8, 6, 9, 4, 7, 10]\n",
    "    detectability = [6, 5, 4, 7, 5, 3]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(stages))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, attack_surface, width, label='Attack Surface Size',\n",
    "                   color='#e74c3c', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, detectability, width, label='Attack Detectability',\n",
    "                   color='#3498db', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('ML Pipeline Stage', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score (0-10)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Attack Surface Analysis Across ML Pipeline', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stages)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim([0, 11])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attack_surface_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Key Insights:\")\n",
    "    print(\"  ‚Ä¢ Inference stage has HIGHEST attack surface (score: 10)\")\n",
    "    print(\"  ‚Ä¢ Training stage attacks are HARDEST to detect (score: 4)\")\n",
    "    print(\"  ‚Ä¢ Validation provides BEST detection opportunity (score: 7)\")\n",
    "    print(\"  ‚Ä¢ Multi-stage defense strategy is essential\\n\")\n",
    "\n",
    "plot_attack_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d14e40",
   "metadata": {},
   "source": [
    "## **6. Real-World Case Studies** <a name=\"case-studies\"></a>\n",
    "\n",
    "### **Case Study 1: Autonomous Vehicle Adversarial Attack**\n",
    "\n",
    "**Scenario:** Stop sign misclassification attack\n",
    "\n",
    "**Attack Details:**\n",
    "- Physical adversarial patches on stop signs\n",
    "- Classifier misidentifies as \"Speed Limit 45\"\n",
    "- Attack succeeds at multiple angles/distances\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Black-box (no model access)\n",
    "- **Attack Type:** Physical evasion attack\n",
    "- **CIA Violation:** Integrity\n",
    "- **Impact:** Safety-critical failure\n",
    "\n",
    "**Lessons:**\n",
    "1. Physical-world attacks are feasible\n",
    "2. Black-box attacks can be effective\n",
    "3. Safety-critical systems need robust defenses\n",
    "\n",
    "---\n",
    "\n",
    "### **Case Study 2: Microsoft Tay Chatbot Poisoning**\n",
    "\n",
    "**Scenario:** Online learning chatbot corrupted via user interactions\n",
    "\n",
    "**Attack Details:**\n",
    "- Users repeatedly fed offensive content\n",
    "- Model learned and reproduced toxic behavior\n",
    "- Bot taken offline within 24 hours\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Data poisoning via normal interface\n",
    "- **Attack Type:** Training-time poisoning\n",
    "- **CIA Violation:** Integrity\n",
    "- **Impact:** Reputational damage, service shutdown\n",
    "\n",
    "**Lessons:**\n",
    "1. Online learning is highly vulnerable\n",
    "2. User-generated data requires validation\n",
    "3. Content filtering is essential\n",
    "4. Human oversight needed for public-facing AI\n",
    "\n",
    "---\n",
    "\n",
    "### **Case Study 3: Netflix Prize Privacy Breach**\n",
    "\n",
    "**Scenario:** Re-identification of users from anonymized ratings\n",
    "\n",
    "**Attack Details:**\n",
    "- Researchers cross-referenced Netflix data with IMDb\n",
    "- Successfully identified users from \"anonymous\" dataset\n",
    "- Revealed sensitive viewing preferences\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Public dataset\n",
    "- **Attack Type:** Privacy attack via linkage\n",
    "- **CIA Violation:** Confidentiality\n",
    "- **Impact:** Privacy violation, lawsuit\n",
    "\n",
    "**Lessons:**\n",
    "1. Anonymization alone is insufficient\n",
    "2. Auxiliary information enables re-identification\n",
    "3. Differential privacy needed for public release\n",
    "4. Privacy risks in seemingly safe data sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize case studies\n",
    "case_studies = pd.DataFrame({\n",
    "    'Case Study': [\n",
    "        'Stop Sign Attack',\n",
    "        'Tay Chatbot',\n",
    "        'Netflix Prize'\n",
    "    ],\n",
    "    'Domain': [\n",
    "        'Autonomous Vehicles',\n",
    "        'Conversational AI',\n",
    "        'Recommender Systems'\n",
    "    ],\n",
    "    'Attack Type': [\n",
    "        'Physical Evasion',\n",
    "        'Data Poisoning',\n",
    "        'Privacy Linkage'\n",
    "    ],\n",
    "    'CIA Violated': [\n",
    "        'Integrity',\n",
    "        'Integrity',\n",
    "        'Confidentiality'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'Safety-Critical',\n",
    "        'Reputational',\n",
    "        'Privacy Breach'\n",
    "    ],\n",
    "    'Year': [2017, 2016, 2007],\n",
    "    'Key Lesson': [\n",
    "        'Physical attacks feasible',\n",
    "        'Online learning vulnerable',\n",
    "        'Anonymization insufficient'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"REAL-WORLD AI SECURITY INCIDENTS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(case_studies.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Timeline visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "years = case_studies['Year'].values\n",
    "names = case_studies['Case Study'].values\n",
    "colors_map = {'Integrity': '#e74c3c', 'Confidentiality': '#3498db'}\n",
    "colors = [colors_map[x] for x in case_studies['CIA Violated']]\n",
    "\n",
    "ax.scatter(years, [1]*len(years), s=500, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for year, name, y_offset in zip(years, names, [0.15, -0.15, 0.15]):\n",
    "    ax.annotate(name, (year, 1), (year, 1 + y_offset),\n",
    "               fontsize=10, ha='center', fontweight='bold',\n",
    "               arrowprops=dict(arrowstyle='->', lw=1.5))\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Timeline of Notable AI Security Incidents', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0.5, 1.5])\n",
    "ax.set_yticks([])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb887",
   "metadata": {},
   "source": [
    "## **7. Defense Strategy Framework** <a name=\"defense\"></a>\n",
    "\n",
    "### **Defense-in-Depth for ML Systems**\n",
    "\n",
    "No single defense is sufficient. Layer multiple protections:\n",
    "\n",
    "#### **Layer 1: Data Protection**\n",
    "- Input validation and sanitization\n",
    "- Outlier detection\n",
    "- Data provenance tracking\n",
    "- Adversarial example detection\n",
    "\n",
    "#### **Layer 2: Model Hardening**\n",
    "- Adversarial training\n",
    "- Certified defenses\n",
    "- Regularization techniques\n",
    "- Ensemble methods\n",
    "\n",
    "#### **Layer 3: Privacy Protection**\n",
    "- Differential privacy\n",
    "- Federated learning\n",
    "- Homomorphic encryption\n",
    "- Secure multi-party computation\n",
    "\n",
    "#### **Layer 4: Monitoring & Detection**\n",
    "- Anomaly detection\n",
    "- Behavioral analysis\n",
    "- Performance monitoring\n",
    "- Audit logging\n",
    "\n",
    "#### **Layer 5: Incident Response**\n",
    "- Model rollback capabilities\n",
    "- Attack mitigation procedures\n",
    "- Forensic analysis\n",
    "- Recovery protocols\n",
    "\n",
    "---\n",
    "\n",
    "### **Defense Selection Matrix**\n",
    "\n",
    "| Attack Type | Primary Defense | Secondary Defense | Detection Method |\n",
    "|-------------|----------------|-------------------|------------------|\n",
    "| Evasion | Adversarial Training | Input Validation | Anomaly Detection |\n",
    "| Data Poisoning | Outlier Removal | RONI Testing | Statistical Testing |\n",
    "| Backdoor | Neural Cleanse | Fine-tuning | Trigger Detection |\n",
    "| Model Inversion | Differential Privacy | Output Perturbation | Query Monitoring |\n",
    "| Sponge Attack | Inference Timeout | Input Filtering | Resource Monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baddfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defense-in-Depth Visualization\n",
    "def visualize_defense_layers():\n",
    "    \"\"\"Create layered defense visualization.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Define concentric circles for defense layers\n",
    "    layers = [\n",
    "        {'radius': 5, 'label': 'Data Protection', 'color': '#3498db'},\n",
    "        {'radius': 4, 'label': 'Model Hardening', 'color': '#2ecc71'},\n",
    "        {'radius': 3, 'label': 'Privacy Protection', 'color': '#f39c12'},\n",
    "        {'radius': 2, 'label': 'Monitoring', 'color': '#e74c3c'},\n",
    "        {'radius': 1, 'label': 'Core Model', 'color': '#9b59b6'}\n",
    "    ]\n",
    "    \n",
    "    for layer in layers:\n",
    "        circle = plt.Circle((0, 0), layer['radius'], \n",
    "                           color=layer['color'], alpha=0.3, \n",
    "                           linewidth=3, edgecolor='black')\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # Add label\n",
    "        angle = np.pi / 4\n",
    "        x = layer['radius'] * 0.7 * np.cos(angle)\n",
    "        y = layer['radius'] * 0.7 * np.sin(angle)\n",
    "        ax.text(x, y, layer['label'], fontsize=11, fontweight='bold',\n",
    "               ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', edgecolor='black'))\n",
    "    \n",
    "    ax.set_xlim([-6, 6])\n",
    "    ax.set_ylim([-6, 6])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Defense-in-Depth for ML Systems', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add attack arrows\n",
    "    attack_angles = [0, np.pi/2, np.pi, 3*np.pi/2]\n",
    "    attack_names = ['Evasion', 'Poisoning', 'Privacy', 'Sponge']\n",
    "    \n",
    "    for angle, name in zip(attack_angles, attack_names):\n",
    "        start_x = 5.5 * np.cos(angle)\n",
    "        start_y = 5.5 * np.sin(angle)\n",
    "        end_x = 0.8 * np.cos(angle)\n",
    "        end_y = 0.8 * np.sin(angle)\n",
    "        \n",
    "        ax.annotate('', xy=(end_x, end_y), xytext=(start_x, start_y),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "        ax.text(start_x * 1.15, start_y * 1.15, name, fontsize=10,\n",
    "               ha='center', va='center', color='red', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('defense_in_depth.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_defense_layers()\n",
    "print(\"\\n‚úì Defense-in-Depth visualization created!\")\n",
    "print(\"\\nüí° Key Principle: Multiple layers provide redundancy\")\n",
    "print(\"   If one defense fails, others still protect the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be431db",
   "metadata": {},
   "source": [
    "## **8. Exercises** <a name=\"exercises\"></a>\n",
    "\n",
    "### **Exercise 1: Threat Modeling Practice (Medium)**\n",
    "\n",
    "Choose a real-world ML system:\n",
    "- Facial recognition system\n",
    "- Credit card fraud detection\n",
    "- Spam email filter\n",
    "- Content recommendation engine\n",
    "\n",
    "Create a complete threat model:\n",
    "1. Define system boundaries and assets\n",
    "2. Identify 5-7 potential threats\n",
    "3. Assess likelihood and impact for each\n",
    "4. Create a risk matrix\n",
    "5. Propose defense strategies\n",
    "\n",
    "**Deliverable:** Use the `ThreatModel` class to document your analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2: Attack Classification (Easy)**\n",
    "\n",
    "For each scenario, classify the attack:\n",
    "\n",
    "**Scenario A:** An attacker adds imperceptible noise to images to fool an image classifier.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "**Scenario B:** A malicious insider corrupts 5% of training labels in a dataset.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "**Scenario C:** An attacker queries a model repeatedly to reconstruct its decision boundary.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 3: Defense Design (Hard)**\n",
    "\n",
    "Design a multi-layered defense strategy for a medical diagnosis AI system that:\n",
    "1. Processes patient data (sensitive)\n",
    "2. Provides treatment recommendations\n",
    "3. Must be highly accurate and trustworthy\n",
    "4. Faces threats from multiple adversaries\n",
    "\n",
    "Requirements:\n",
    "- Address all three CIA properties\n",
    "- Include 3+ defense layers\n",
    "- Specify detection mechanisms\n",
    "- Consider regulatory compliance (HIPAA)\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 4: Case Study Analysis (Medium)**\n",
    "\n",
    "Research the **ClearView AI privacy controversy**:\n",
    "1. What attack/vulnerability was exploited?\n",
    "2. Which CIA principle was violated?\n",
    "3. What was the impact?\n",
    "4. How could it have been prevented?\n",
    "5. What defenses would you recommend?\n",
    "\n",
    "**Format:** 2-page analysis with threat model and defense recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb45088",
   "metadata": {},
   "source": [
    "## **9. Conclusion** <a name=\"conclusion\"></a>\n",
    "\n",
    "### **What You Learned**\n",
    "\n",
    "- **CIA Triad:** How confidentiality, integrity, and availability apply to ML  \n",
    "- **Attack Taxonomy:** Classification by timing, objective, and access  \n",
    "- **Threat Modeling:** Systematic analysis of adversarial risks  \n",
    "- **Attack Surface:** Vulnerability points across ML pipeline  \n",
    "- **Defense Strategies:** Multi-layered protection approaches  \n",
    "- **Real-World Context:** Case studies from actual incidents  \n",
    "\n",
    "### **Key Principles**\n",
    "\n",
    "1. **No Perfect Defense:** Security is about risk management, not elimination\n",
    "2. **Context Matters:** Threat models vary by application domain\n",
    "3. **Defense-in-Depth:** Multiple layers provide resilience\n",
    "4. **Trade-offs:** Security often conflicts with accuracy/performance\n",
    "5. **Evolving Threats:** Continuous monitoring and adaptation required\n",
    "\n",
    "### **Preparing for Upcoming Labs**\n",
    "\n",
    "Now that you understand threat modeling, you're ready to:\n",
    "\n",
    "**Module 2:** Implement and defend against evasion attacks  \n",
    "**Module 3-4:** Execute and detect poisoning attacks  \n",
    "**Module 5:** Create and mitigate sponge attacks  \n",
    "**Module 6:** Launch and prevent privacy attacks  \n",
    "**Module 7:** Generate and evaluate synthetic data  \n",
    "**Module 8:** Deploy comprehensive defense systems  \n",
    "\n",
    "Each subsequent lab will reference this threat modeling framework.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Resources**\n",
    "\n",
    "**Foundational Papers:**\n",
    "- [Adversarial Examples Are Not Bugs, They Are Features (Ilyas et al., 2019)](https://arxiv.org/abs/1905.02175)\n",
    "- [SoK: Security and Privacy in Machine Learning (Papernot et al., 2018)](https://ieeexplore.ieee.org/document/8406613)\n",
    "- [The NIST Adversarial ML Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)\n",
    "\n",
    "**Industry Standards:**\n",
    "- MITRE ATLAS: Adversarial Threat Landscape for AI Systems\n",
    "- OWASP Machine Learning Security Top 10\n",
    "- ISO/IEC 24029: AI Trustworthiness\n",
    "\n",
    "**Tools & Frameworks:**\n",
    "- [Microsoft Threat Modeling Tool](https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling)\n",
    "- [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privaudit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
