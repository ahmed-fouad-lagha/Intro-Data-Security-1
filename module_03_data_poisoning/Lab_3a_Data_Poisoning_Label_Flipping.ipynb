{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1aaea3",
   "metadata": {},
   "source": [
    "# **Lab 3a: Data Poisoning Attacks (Label Flipping & Sample Injection)**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr.  \n",
    "**Module 3:** Integrity Attacks - Data Poisoning  \n",
    "**Estimated Time:** 120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Implement** label-flipping poisoning attacks\n",
    "2. **Craft** targeted poison samples\n",
    "3. **Measure** poison success via misclassification rates\n",
    "4. **Analyze** impact vs. poison budget\n",
    "5. **Design** defenses against poisoning\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Label-Flip Poisoning](#label-flip)\n",
    "3. [Targeted Poisoning](#targeted)\n",
    "4. [Poison Budget Analysis](#budget)\n",
    "5. [Detection & Defense](#defense)\n",
    "6. [Exercises](#exercises)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab474caf",
   "metadata": {},
   "source": [
    "## **Setup & Imports** <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36092b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beab44f",
   "metadata": {},
   "source": [
    "## **Label-Flip Poisoning** <a name=\"label-flip\"></a>\n",
    "\n",
    "In label-flip poisoning, the attacker flips the labels of a subset of training samples.\n",
    "\n",
    "**Goal:** Degrade model accuracy or create specific misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67625c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_flip_poison(dataset, flip_fraction=0.1, target_class=None):\n",
    "    \"\"\"\n",
    "    Create poisoned dataset by flipping labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset: original dataset\n",
    "        flip_fraction: fraction of samples to flip (0.0-1.0)\n",
    "        target_class: if None, flip random labels; else flip to target_class\n",
    "    \"\"\"\n",
    "    # Copy dataset\n",
    "    poisoned_data = [(x, y) for x, y in dataset]\n",
    "    \n",
    "    # Select indices to poison\n",
    "    n_poison = int(len(poisoned_data) * flip_fraction)\n",
    "    poison_indices = np.random.choice(len(poisoned_data), n_poison, replace=False)\n",
    "    \n",
    "    poison_count = 0\n",
    "    for idx in poison_indices:\n",
    "        x, y = poisoned_data[idx]\n",
    "        if target_class is not None:\n",
    "            new_label = target_class\n",
    "        else:\n",
    "            # Random wrong label\n",
    "            new_label = np.random.randint(0, 10)\n",
    "            while new_label == y:\n",
    "                new_label = np.random.randint(0, 10)\n",
    "        \n",
    "        poisoned_data[idx] = (x, new_label)\n",
    "        poison_count += 1\n",
    "    \n",
    "    return poisoned_data, poison_indices, poison_count\n",
    "\n",
    "# Create poisoned dataset with 10% label flip\n",
    "poisoned_train, poison_idx, poison_count = create_label_flip_poison(\n",
    "    train_dataset, flip_fraction=0.1, target_class=None\n",
    ")\n",
    "\n",
    "print(f\"Poisoned {poison_count} samples (10% of training data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe931c",
   "metadata": {},
   "source": [
    "## **Targeted Poisoning** <a name=\"targeted\"></a>\n",
    "\n",
    "Target-class flipping: flip all '3' samples to become '8'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targeted_poison(dataset, source_class=3, target_class=8, flip_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Create targeted poisoning: flip source_class → target_class.\n",
    "    \"\"\"\n",
    "    poisoned_data = [(x, y) for x, y in dataset]\n",
    "    \n",
    "    # Find all samples of source class\n",
    "    source_indices = [i for i, (_, y) in enumerate(poisoned_data) if y == source_class]\n",
    "    \n",
    "    # Poison a fraction of them\n",
    "    n_poison = int(len(source_indices) * flip_fraction)\n",
    "    poison_indices = np.random.choice(source_indices, n_poison, replace=False)\n",
    "    \n",
    "    for idx in poison_indices:\n",
    "        x, _ = poisoned_data[idx]\n",
    "        poisoned_data[idx] = (x, target_class)\n",
    "    \n",
    "    return poisoned_data, poison_indices\n",
    "\n",
    "# Create targeted poison: 3 → 8\n",
    "poisoned_train_targeted, poison_idx_targeted = create_targeted_poison(\n",
    "    train_dataset, source_class=3, target_class=8, flip_fraction=0.5\n",
    ")\n",
    "\n",
    "print(f\"Poisoned {len(poison_idx_targeted)} samples of class 3 to class 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a5993",
   "metadata": {},
   "source": [
    "## **Poison Budget Analysis** <a name=\"budget\"></a>\n",
    "\n",
    "How does poison effectiveness scale with the fraction of poisoned data?\n",
    "\n",
    "**Note (fast mode):** This loop can be slow on full MNIST. We limit training to a subset and cap batches per epoch for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Quick training function\n",
    "def train_epoch(model, loader, optimizer, device, max_batches=None):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "print(\"Training utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac54c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze poison effectiveness vs. budget\n",
    "poison_fractions = [0.0, 0.05, 0.1, 0.2, 0.3]\n",
    "results = {'poison_fraction': [], 'clean_accuracy': [], 'poisoned_accuracy': []}\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Fast-mode settings\n",
    "max_train_samples = 8000\n",
    "max_batches = 80\n",
    "\n",
    "def make_subset(data, n):\n",
    "    n = min(n, len(data))\n",
    "    indices = np.random.choice(len(data), n, replace=False)\n",
    "    if isinstance(data, list):\n",
    "        return [data[i] for i in indices]\n",
    "    return Subset(data, indices)\n",
    "\n",
    "for frac in poison_fractions:\n",
    "    print(f\"\\nTraining with poison fraction: {frac}\")\n",
    "    \n",
    "    # Create poisoned data\n",
    "    if frac == 0:\n",
    "        train_data = train_dataset\n",
    "    else:\n",
    "        train_data, _, _ = create_label_flip_poison(train_dataset, flip_fraction=frac)\n",
    "    \n",
    "    # Subsample for speed\n",
    "    train_data = make_subset(train_data, max_train_samples)\n",
    "    \n",
    "    # Train model\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    \n",
    "    for epoch in range(3):  # Quick 3 epochs\n",
    "        train_epoch(model, train_loader, optimizer, device, max_batches=max_batches)\n",
    "    \n",
    "    acc = evaluate(model, test_loader, device)\n",
    "    results['poison_fraction'].append(frac)\n",
    "    results['clean_accuracy'].append(acc)\n",
    "    print(f\"Test accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1b08c",
   "metadata": {},
   "source": [
    "### **Visualize Poison Impact**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f031c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot poison effectiveness\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['poison_fraction'], [acc * 100 for acc in results['clean_accuracy']], \n",
    "         marker='o', linewidth=2, markersize=8, label='Accuracy with Label-Flip Poison')\n",
    "plt.axhline(y=98, color='green', linestyle='--', linewidth=2, label='Clean Model')\n",
    "plt.xlabel('Poison Fraction', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Impact of Label-Flip Poisoning on Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea376138",
   "metadata": {},
   "source": [
    "## **Detection & Defense** <a name=\"defense\"></a>\n",
    "\n",
    "### **Strategy 1: Outlier Detection (Isolation Forest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_poison_outliers(dataset, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Use Isolation Forest to detect anomalous (potentially poisoned) samples.\n",
    "    \"\"\"\n",
    "    # Flatten images\n",
    "    X = np.array([x.numpy().flatten() for x, _ in dataset])\n",
    "    \n",
    "    # Fit Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    predictions = iso_forest.fit_predict(X)\n",
    "    \n",
    "    # -1 = anomaly, 1 = normal\n",
    "    anomaly_indices = np.where(predictions == -1)[0]\n",
    "    \n",
    "    return anomaly_indices, iso_forest\n",
    "\n",
    "# Detect poison in poisoned dataset\n",
    "anomaly_idx, iso_forest = detect_poison_outliers(\n",
    "    [(x, y) for x, y in poisoned_train], contamination=0.1\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(anomaly_idx)} anomalies (expected ~6000 for 10% poison)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de578581",
   "metadata": {},
   "source": [
    "### **Strategy 2: Confident Learning (Label Cleaning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90528224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suspicious_labels(model, dataset, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Identify samples where model prediction disagrees with label.\n",
    "    High disagreement suggests label corruption.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    suspicious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(dataset[:1000]):  # Check first 1000\n",
    "            x_tensor = x.unsqueeze(0).to(device)\n",
    "            logits = model(x_tensor)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred_prob = probs[0, y].item()\n",
    "            \n",
    "            # Low confidence in true label = suspicious\n",
    "            if pred_prob < threshold:\n",
    "                suspicious.append((idx, y, pred_prob))\n",
    "    \n",
    "    return suspicious\n",
    "\n",
    "print(\"Label cleaning function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0eb5c",
   "metadata": {},
   "source": [
    "## **Exercises** <a name=\"exercises\"></a>\n",
    "\n",
    "1. **Vary poison fractions:** Test with 1%, 5%, 15%, 25% and plot the accuracy curve.\n",
    "2. **Compare targeted vs. random:** Which is more effective? Why?\n",
    "3. **Defense evaluation:** Which detection method (Isolation Forest vs. Confident Learning) works better?\n",
    "4. **Poisoning different classes:** Does poisoning class \"3\" vs. class \"0\" have different effects?\n",
    "5. **Data cleaning:** Implement a defense that removes suspected poison and retrains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539591d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Course:** Introduction to Data Security Pr.  \n",
    "**Module 3a Completed** ✓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
